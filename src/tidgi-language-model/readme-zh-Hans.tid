title: $:/plugins/linonetwo/tidgi-language-model/readme/zh-Hans
type: text/vnd.tiddlywiki

在 TiddlyWiki 中使用 LLaMa。

你的侧边栏会多出一个 TG AI 页面，可以直接进行对话，对话的历史会保存。如要清除历史记录，则直接删除 `history` 参数指向的条目。

LLaMa 实际上是一个微件，你可以按照自己的需求定制聊天机器人：

```html
<$tidgi-chat />
```

还可以添加各种可选参数来定制行为：

|!参数 |!解释 |
|history |填写一个条目的标题，用于持久化存储聊天记录 |
|component |微件的DOM标签类型，默认为div |
|className |微件的类名，用于自定义样式 |
|readonly |如果为readonly，则不会出现对话输入框，配合history参数仅做展示用 |
|system_message |系统消息，用于AI的行为，例如"你是一个经验丰富的律师" |

除此之外，还支持如下 LLaMa 参数：

```ts
interface Generate {
  nThreads: number
  nTokPredict: number
  /**
   * logit bias for specific tokens
   * Default: None
   */
  logitBias?: Array<LogitBias>
  /**
   * top k tokens to sample from
   * Range: <= 0 to use vocab size
   * Default: 40
   */
  topK?: number
  /**
   * top p tokens to sample from
   * Default: 0.95
   * 1.0 = disabled
   */
  topP?: number
  /**
   * tail free sampling
   * Default: 1.0
   * 1.0 = disabled
   */
  tfsZ?: number
  /**
   * temperature
   * Default: 0.80
   * 1.0 = disabled
   */
  temp?: number
  /**
   * locally typical sampling
   * Default: 1.0
   * 1.0 = disabled
   */
  typicalP?: number
  /**
   * repeat penalty
   * Default: 1.10
   * 1.0 = disabled
   */
  repeatPenalty?: number
  /**
   * last n tokens to penalize
   * Default: 64
   * 0 = disable penalty, -1 = context size
   */
  repeatLastN?: number
  /**
   * frequency penalty
   * Default: 0.00
   * 1.0 = disabled
   */
  frequencyPenalty?: number
  /**
   * presence penalty
   * Default: 0.00
   * 1.0 = disabled
   */
  presencePenalty?: number
  /**
   * Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
   * Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity
   * Default: 0
   * 0 = disabled
   * 1 = mirostat 1.0
   * 2 = mirostat 2.0
   */
  mirostat?: number
  /**
   * The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
   * Default: 5.0
   */
  mirostatTau?: number
  /**
   * The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
   * Default: 0.1
   */
  mirostatEta?: number
  /**
   * stop sequence
   * Default: None
   */
  stopSequence?: string
  /**
   * consider newlines as a repeatable token
   * Default: true
   */
  penalizeNl?: boolean
  /** prompt */
  prompt: string
}
```


或 RWKV 参数

```ts
interface RwkvInvocation {
  maxPredictLength: number
  topP: number
  temp: number
  endToken?: number
  endString?: string
  seed?: number
  prompt: string
  isSkipGeneration?: boolean
  sessionFilePath?: string
  isOverwriteSessionFile?: boolean
  presencePenalty?: number
  frequencyPenalty?: number
}
```

其具体用法可以查看[[官方文档|https://llama-node.vercel.app/docs/backends/llama.cpp/inference]]。

现在还没有做多轮对话，即便是在一个微件里聊的，也都是单轮对话，多轮对话等下个版本再搞。

!! 高级

如果在微件中嵌套自己的 action，就可以在对话完成时拿到回答的结果，这需要你知道该如何编写一个支持 action 的微件。输出的结果保存在 `output-text` 变量中。

同时，在对话完成时你也可以捕获到微件的冒泡事件，以及全局事件，二者分别使用`addEventListener`和`$tw.hooks.addHook`(事件名称就是`tidgi-chat`)来实现。如下是事件负载的类型定义：