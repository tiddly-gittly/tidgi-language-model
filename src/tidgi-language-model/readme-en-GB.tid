title: $:/plugins/linonetwo/tidgi-language-model/readme/en-GB
type: text/vnd.tiddlywiki

Using LLaMa in TiddlyWiki.

You will have an additional "TG AI" page in your sidebar, where you can have a conversation directly, and the history of the conversation will be saved. To clear the history, simply delete the entry pointed to by the `history` parameter.

LLaMa is actually a widget that allows you to customize the chatbot according to your needs:

```html
<$tidgi-chat />
```

Various optional parameters can also be added to customize the behavior.

|!Attributes |!Explanation |
|history |Fill in an tiddler title for persistent storage of chat logs |
|scroll |If yes, the conversation record can be scrolled up and down, but the height must be specified in the outer layer of the widget, refer to the [[sidebar|$:/plugins/linonetwo/tidgi-language-model/side-bar]] writing |
|component |DOM tag type for microware, default is div |
|className |Class name of the widget for custom styles |
|readonly |If it is readonly, no dialog input box will appear, and it will be used for display only with the history parameter. |
|system_message |System messages to customize the AI's behavior, such as "You are an experienced lawyer" |

In addition, the following LLaMa parameters are supported:

```ts
interface Generate {
  nThreads: number
  nTokPredict: number
  /**
   * logit bias for specific tokens
   * Default: None
   */
  logitBias?: Array<LogitBias>
  /**
   * top k tokens to sample from
   * Range: <= 0 to use vocab size
   * Default: 40
   */
  topK?: number
  /**
   * top p tokens to sample from
   * Default: 0.95
   * 1.0 = disabled
   */
  topP?: number
  /**
   * tail free sampling
   * Default: 1.0
   * 1.0 = disabled
   */
  tfsZ?: number
  /**
   * temperature
   * Default: 0.80
   * 1.0 = disabled
   */
  temp?: number
  /**
   * locally typical sampling
   * Default: 1.0
   * 1.0 = disabled
   */
  typicalP?: number
  /**
   * repeat penalty
   * Default: 1.10
   * 1.0 = disabled
   */
  repeatPenalty?: number
  /**
   * last n tokens to penalize
   * Default: 64
   * 0 = disable penalty, -1 = context size
   */
  repeatLastN?: number
  /**
   * frequency penalty
   * Default: 0.00
   * 1.0 = disabled
   */
  frequencyPenalty?: number
  /**
   * presence penalty
   * Default: 0.00
   * 1.0 = disabled
   */
  presencePenalty?: number
  /**
   * Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
   * Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity
   * Default: 0
   * 0 = disabled
   * 1 = mirostat 1.0
   * 2 = mirostat 2.0
   */
  mirostat?: number
  /**
   * The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
   * Default: 5.0
   */
  mirostatTau?: number
  /**
   * The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
   * Default: 0.1
   */
  mirostatEta?: number
  /**
   * stop sequence
   * Default: None
   */
  stopSequence?: string
  /**
   * consider newlines as a repeatable token
   * Default: true
   */
  penalizeNl?: boolean
  /** prompt */
  prompt: string
}
```


or RWKV parameters:

```ts
interface RwkvInvocation {
  maxPredictLength: number
  topP: number
  temp: number
  endToken?: number
  endString?: string
  seed?: number
  prompt: string
  isSkipGeneration?: boolean
  sessionFilePath?: string
  isOverwriteSessionFile?: boolean
  presencePenalty?: number
  frequencyPenalty?: number
}
```

Its specific usage can check the [[official documentation|https://llama-node.vercel.app/docs/backends/llama.cpp/inference]].

Now there is no multi-round dialogue, even in a micro-piece chat, but also a single round of dialogue, multi-round dialogue and so on the next version to engage.

!! Advance

If you nest your own action in the widget, you can get the result of the answer when the conversation is completed, which requires that you know how to write a widget that supports actions. The output is stored in the `output-text` variable.

At the same time, you can also catch bubbling events of the widget when the conversation completes, as well as global events, both using `addEventListener` and `$tw.hooks.addHook` (the event name is `tidgi-chat`) respectively. The following is the type definition of the event load.
